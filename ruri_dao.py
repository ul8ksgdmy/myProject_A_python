from pymongo import MongoClient
from ruri_connect import ConnectTo
from ruri_config import Config
import json
import os

import platform

class CrwalingDAO:
    #load db info
    def setdbinfo(self):
        host = "" # linux구분
        if platform.system() != "Linux":
            host = 'host'
        else:
            host = 'lhost'

        # ini파일을 이용해 접속 데이터 읽기
        config = Config()
        data = config.read_info_in_config('mongoDB')
        cnct = ConnectTo(data[host],data['port'],data['database'],data['collection']) #인스턴스화
        cnct.MongoDB() #mongoDB 접속, 현재는 mongoDB만 가능하지만 추후 다른 DB도 선택할 수 있도록 변경
        return cnct


    def insertone(self, cr):
        innerurl = os.path.join(os.path.relpath(os.path.dirname(__file__)))
        conn = self.setdbinfo() #접속값을 받아옴.
        ### 자료를 얼마나 모았을까? ###
        print('titles this crawler has collected till now : ', len(cr[2]))
        with open(innerurl, 'w') as f:
            print('데이터 백업을 위해 %s파일을 호출합니다 : ' % f.name)
            for i in range(0, len(cr[2])):
                ruri = {
                    'no' : cr[0][i],
                    'html' : cr[1][i],
                    'title' : cr[2][i],
                    'thumbup' : cr[3][i],
                    'thumbdown' : cr[4][i],
                    'date' : cr[5][i],
                    'content' : cr[6][i]
                }
                json.dump(ruri, f)
                doc_id = conn.m_collection.insert_one(ruri).inserted_id
                print('no.', i+1, 'inserted id in mongodb : ', doc_id)

        conn.m_client.close()